{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "doAgp1etpKLh",
        "GpKMiHBHwxJf"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkdk22/123/blob/master/DL_Assignment3_Clasification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DL Assignment 3\n",
        "## Group 4\n",
        "**AUTHORS**:\n",
        "*   Dario Mameli [dario.mameli@ugent.be]\n",
        "*   Àngel Masip LLopis [angel.masipllopis@ugent.be]\n",
        "*   Michele Russo [michele.russo@ugent.be]\n",
        "\n",
        "\n",
        "\n",
        "This notebook is to be intended as both report and code.\n",
        "\n",
        "Running on google colab is suggested to make sure all required libraries and packages are present."
      ],
      "metadata": {
        "id": "FRPrCmqA3ZTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GitHub\n",
        "\n",
        "Let's clone the repo with the dataset"
      ],
      "metadata": {
        "id": "MIZ_-9-u6E7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dkdk22/DL_Ass3\n",
        "import sys\n",
        "# Add the repository to the path\n",
        "sys.path.insert(1, '/content/DL_Ass3/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz0pecYrli1O",
        "outputId": "dc38fc06-ca35-4299-fb37-93d7cdc98299"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DL_Ass3' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzIrHK0HnwaT"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Firstly let's setup the workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pip Installs"
      ],
      "metadata": {
        "id": "VxmIWEgYIL00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest version of TensorFlow, which includes Keras (tf.keras)\n",
        "#!pip install tensorflow --upgrade\n",
        "!pip install keras --upgrade --quiet\n",
        "!pip install keras-tuner --upgrade\n",
        "!pip install PrettyTable"
      ],
      "metadata": {
        "id": "gRSbKIjfILkE",
        "outputId": "8e0e0767-98cf-4449-9b79-346352fb7210",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.9.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.10.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.10.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
            "Requirement already satisfied: PrettyTable in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from PrettyTable) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiSBqqDaizpT"
      },
      "source": [
        "## Packages\n",
        "\n",
        "Let's import all the necessary functions and packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_tndfUaBvYFN"
      },
      "outputs": [],
      "source": [
        "from dl_utils import plot_history\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import keras\n",
        "from keras import regularizers\n",
        "import matplotlib.gridspec as gridspec\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import keras_tuner as kt\n",
        "from keras import backend as backend\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    BatchNormalization,\n",
        "    Conv2D,\n",
        "    Conv2DTranspose,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Flatten,\n",
        "    GlobalAveragePooling2D,\n",
        "    MaxPooling2D,\n",
        "    UpSampling2D\n",
        ")\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import json\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import keras.applications as app\n",
        "from keras.layers import LSTM, Dense, Embedding, Input, GRU, Bidirectional\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import gc\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import zipfile\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Global variables"
      ],
      "metadata": {
        "id": "zg_oAPilI2CG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED =42\n",
        "TRAIN_BASELINE = False\n",
        "TRAIN_MODEL = True"
      ],
      "metadata": {
        "id": "PCYSJedrI4nW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsTb0qM5n7gE"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGwus8LWjP33"
      },
      "source": [
        "Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = 'DL_Ass3/review_553850.zip'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref: #Extract the file\n",
        "    zip_ref.extractall()\n",
        "\n",
        "with open('review_553850.json', 'r') as json_file: # Open the json file\n",
        "    data = json.load(json_file)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for id, review in data[\"reviews\"].items():\n",
        "    review_text = review[\"review\"]\n",
        "    funny_votes = review[\"votes_funny\"]\n",
        "    X.append(review_text)\n",
        "    y.append(funny_votes)"
      ],
      "metadata": {
        "id": "pjelOlrluxfZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preprocessing"
      ],
      "metadata": {
        "id": "dGl0DJoAmnus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to do the same preprocessing as in the other notebook. We are going to eliminate stopwords and also we are going to use steeming, tokenize and pad the sentences.\n"
      ],
      "metadata": {
        "id": "-NEwlSBGwIrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "# Remove stopwords, convert to lowercase and stemming\n",
        "X_prepro = []\n",
        "for text in X:\n",
        "    words = text.split()\n",
        "\n",
        "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in filtered_words]\n",
        "    preprocessed_text = ' '.join(stemmed_tokens)\n",
        "    X_prepro.append(preprocessed_text)\n",
        "# Remove the reviews with 0 words\n",
        "X_filtered = []\n",
        "y_filtered = []\n",
        "for review, label in zip(X_prepro, y):\n",
        "    if len(review.split()) > 0:\n",
        "        X_filtered.append(review)\n",
        "        y_filtered.append(label)\n",
        "print(\"Length of the dataset: \"+str(len(X_filtered)))\n",
        "# Tokenize\n",
        "num_words=10000\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(X_filtered)\n",
        "X_sequences = tokenizer.texts_to_sequences(X_filtered)\n",
        "# Initialize variables to store information about the longest sequence\n",
        "max_length = 0\n",
        "max_index = -1\n",
        "\n",
        "# Iterate through the list of sequences to have the longes sequence\n",
        "for i, sequence in enumerate(X_sequences):\n",
        "    length = len(sequence)\n",
        "    # Update max_length and max_index if the current sequence is longer\n",
        "    if length > max_length:\n",
        "        max_length = length\n",
        "        max_index = i\n",
        "\n",
        "# Retrieve the longest sequence\n",
        "longest_sequence = X_sequences[max_index]\n",
        "\n",
        "# Print the longest sequence\n",
        "print(\"The longest sequence:\")\n",
        "print(longest_sequence)\n",
        "print(len(longest_sequence))\n",
        "# Padding\n",
        "max_sequence_length = len(longest_sequence)\n",
        "padded_sequences = pad_sequences(X_sequences, maxlen=max_sequence_length)\n",
        "# Convert to np arrays\n",
        "scaler = StandardScaler()\n",
        "normalized_data = scaler.fit_transform(padded_sequences)\n",
        "X = np.array(normalized_data)\n",
        "print(X.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KO92gjnn0sL",
        "outputId": "714bb09b-e3be-42d6-dce8-fe4f5258db5f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the dataset: 188843\n",
            "The longest sequence:\n",
            "[169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154]\n",
            "1600\n",
            "(188843, 1600)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Binary classification problem"
      ],
      "metadata": {
        "id": "6GLrF8Ncrkik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that this is a really difficoult task to learn because most of the reviews have 0 funny_votes and some of them have a really high value of funny_votes. That's why we are going to change the approach and convert the problem to a binary classification problem, that should be easier to learn to our model."
      ],
      "metadata": {
        "id": "avBowkvxrqPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to convert it by fixing an number of funny_votes N, treating a review as funny or not funny depending on whether it has more or fewer votes than N. In our case N is going to be 1 because of the unbalancement of the dataset."
      ],
      "metadata": {
        "id": "R0df2-SXsKod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = [0 if num == 0 else 1 for num in y_filtered]\n",
        "y = np.array(y)\n",
        "counts = [0,0]\n",
        "counts[0] = np.sum(data == 0)\n",
        "counts[1] = np.sum(data == 1)"
      ],
      "metadata": {
        "id": "WjWCPIAcQc1q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline Model"
      ],
      "metadata": {
        "id": "ATIPmZsZNe9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = GradientBoostingClassifier()"
      ],
      "metadata": {
        "id": "6madG_8ENiPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Best model for classification problem"
      ],
      "metadata": {
        "id": "zLZVRqSVBapQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperModelBuilderBinary(kt.HyperModel):\n",
        "    def __init__(self, max_sequence_length, loss_function, max_features):\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.loss_function = loss_function\n",
        "        self.max_features = max_features\n",
        "\n",
        "    def build_lstm_model(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=self.max_features, output_dim=hp.Int('embedding_dim', min_value=128, max_value=128, step=32)))  #fix embedding size?\n",
        "        model.add(LSTM(units=hp.Int('units', min_value=32, max_value=256, step=32), #https://keras.io/api/layers/recurrent_layers/lstm/\n",
        "                       dropout=hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1),\n",
        "                       recurrent_dropout=hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1), #reduce two dropouts\n",
        "                       input_shape=(self.max_sequence_length, 1)))\n",
        "        model.add(Dense(1, activation=\"sigmoid\"))\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])),\n",
        "                      loss=self.loss_function,\n",
        "                      metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "    def build_bidirectional_lstm_model(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=self.max_features, output_dim=hp.Int('embedding_dim', min_value=128, max_value=128, step=32)))\n",
        "        model.add(Bidirectional(LSTM(units=hp.Int('units', min_value=32, max_value=256, step=32),\n",
        "                                     dropout=hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1),\n",
        "                                     recurrent_dropout=hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1),\n",
        "                                     input_shape=(self.max_sequence_length, 1))))\n",
        "        model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])),\n",
        "                      loss=self.loss_function,\n",
        "                      metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "\n",
        "    def build_gru_model(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=self.max_features, output_dim=hp.Int('embedding_dim', min_value=128, max_value=128, step=32)))\n",
        "        model.add(GRU(units=hp.Int('units', min_value=32, max_value=256, step=32),   #https://keras.io/api/layers/recurrent_layers/gru/\n",
        "                      dropout=hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1),\n",
        "                      recurrent_dropout=hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1),\n",
        "                      input_shape=(self.max_sequence_length, 1)))\n",
        "        model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])),\n",
        "                      loss=self.loss_function,\n",
        "                      metrics=[\"accuracy\"])\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "    def build_model(self,hp):\n",
        "        model_name = hp.Choice('model_name', values=['GRU', 'BidirectionalLSTM', 'LSTM'])\n",
        "        if model_name == 'LSTM':\n",
        "            return self.build_lstm_model(hp)\n",
        "        elif model_name == 'BidirectionalLSTM':\n",
        "            return self.build_bidirectional_lstm_model(hp)\n",
        "        elif model_name == 'GRU':\n",
        "            return self.build_gru_model(hp)\n",
        "\n",
        "    def fit(self, hp, model, *args, **kwargs):\n",
        "        weight_for_0 = 1. / counts[0]\n",
        "        weight_for_1 = 1. / counts[1]\n",
        "        class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "        return model.fit(X_train, y_train, validation_data=(X_val, y_val),class_weight=class_weight)\n",
        "\n",
        "    def summary(self):\n",
        "        return self.model.summary()\n",
        "\n",
        "    def save(self, path):\n",
        "        return self.model.save(path)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.model.predict(x)"
      ],
      "metadata": {
        "id": "UeCOUYOMshVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss = tf.keras.losses.BinaryFocalCrossentropy(\n",
        "    apply_class_balancing=False,\n",
        "    alpha=0.25,\n",
        "    gamma=2.0,\n",
        "    from_logits=False,\n",
        "    label_smoothing=0.0,\n",
        "    axis=-1,\n",
        "    name='binary_focal_crossentropy'\n",
        ")\n",
        "# Define hyperparameters for the tuner\n",
        "hypermodel_builder = HyperModelBuilderBinary(max_sequence_length=max_sequence_length,\n",
        "                                       loss_function=loss, max_features=10000)\n",
        "\n",
        "# Initialize tuner\n",
        "tuner = kt.BayesianOptimization(hypermodel_builder.build_model,\n",
        "                        objective='val_loss',\n",
        "                        max_trials=10,\n",
        "                        executions_per_trial=1, # The documentation says that makes things faster\n",
        "                        overwrite=True,\n",
        "                        max_consecutive_failed_trials=5,\n",
        "                        directory='tuner_results',\n",
        "                        project_name='regression_tuning')\n",
        "\n",
        "# Summarize the search space\n",
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "id": "MILTGt3fBQUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training & Evaluating"
      ],
      "metadata": {
        "id": "gUhGFFdL6RXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_accuracies(X_data, y_data, model):\n",
        "  # The predicted probabilities for each class\n",
        "  y_pred_probs = model.predict(X_data)\n",
        "\n",
        "  # The corresponding predicted labels\n",
        "  y_pred_labels = np.argmax(y_pred_probs, axis=0)\n",
        "\n",
        "  # The corresponding true labels\n",
        "  y_true_labels = np.argmax(y_data, axis=0)\n",
        "\n",
        "  # Compute confusion matrix\n",
        "  cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
        "\n",
        "  # Compute accuracies and weights for each class\n",
        "  class_accuracies = []\n",
        "  for class_label in range(len(cm)):\n",
        "      class_accuracy = cm[class_label, class_label] / np.sum(cm[class_label, :])\n",
        "      class_accuracies.append(class_accuracy)\n",
        "\n",
        "  # Print the classification report\n",
        "  print(classification_report(y_true_labels, y_pred_labels))\n",
        "\n",
        "  return class_accuracies"
      ],
      "metadata": {
        "id": "5j5A0kXPINZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=SEED)"
      ],
      "metadata": {
        "id": "jEF0i1-LPLu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if TRAIN_BASELINE:\n",
        "  baseline_model.fit(X_train, y_train)\n",
        "  joblib.dump(baseline_model, '/content/DL_Ass3/baseline_model_cla.pkl')\n",
        "else:\n",
        "  baseline_model = joblib.load('/content/DL_Ass3/baseline_model_cla.pkl')"
      ],
      "metadata": {
        "id": "loJTWWAG8o0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = baseline_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "  # Print the classification report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "ESdu4ceVBRoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the model is only predicting 0"
      ],
      "metadata": {
        "id": "Gmfees9IJ-HH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train or model"
      ],
      "metadata": {
        "id": "ft2Bh-bGBYZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True,  random_state=SEED)"
      ],
      "metadata": {
        "id": "ymP6xQgl8oKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if TRAIN_MODEL:\n",
        "  # Early stop to prevent overfitting\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=4,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=True,\n",
        "    start_from_epoch=0,\n",
        "  )\n",
        "\n",
        "  # Perform hyperparameter search\n",
        "  tuner.search(X_train, y_train, epochs=1, batch_size=128, callbacks=[stop_early])\n",
        "  # Get the best hyperparameters\n",
        "  best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "  # Build the model\n",
        "  best_model = tuner.hypermodel.build(best_hps)\n",
        "  # Save the model\n",
        "  best_model.save('content/DL_Ass3/best_model_cla.keras')\n",
        "  # Fit the model\n",
        "  history = best_model.fit(X_train, y_train, epochs=50, batch_size=512, validation_data=(X_val, y_val), callbacks=[stop_early])\n",
        "  # Save the model\n",
        "  best_model.save('content/DL_Ass3/best_model_cla.keras')\n",
        "  plot_history(history)\n",
        "else:\n",
        "  best_model = keras.models.load_model('content/DL_Ass3/best_model_cla.keras')\n"
      ],
      "metadata": {
        "id": "UlLEXS2wBVu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V4kA0W1xBUUC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}